# -*- coding: utf-8 -*-
"""AUTO_image.pynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KGC9dEbKV8dmrIYaaZ0amq3NCCBamuCL
"""

import os, requests, zipfile, glob, shutil
import cv2
import numpy as np

DATA_DIR = "./DIV2K_subset"
os.makedirs(DATA_DIR, exist_ok=True)

url = "http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip"
zip_path = "DIV2K_train_HR.zip"

# Download and extract
if not os.path.exists(os.path.join(DATA_DIR, "images")):
    print("Downloading DIV2K (~1GB)... may take a while")
    r = requests.get(url, stream=True)
    with open(zip_path, "wb") as f:
        for chunk in r.iter_content(chunk_size=1024*1024):
            f.write(chunk)
    print("Extracting...")
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall("./DIV2K_raw")
    os.remove(zip_path)

    # Move first 100 images into subset folder
    src_folder = "./DIV2K_raw/DIV2K_train_HR"
    dst_folder = os.path.join(DATA_DIR, "images")
    os.makedirs(dst_folder, exist_ok=True)
    for i, f in enumerate(sorted(glob.glob(os.path.join(src_folder, "*.png")))[:100]):
        shutil.move(f, dst_folder)
    shutil.rmtree("./DIV2K_raw")

print("Dataset prepared.")

# Preprocess into numpy array
IMG_SIZE = 128  # can change to 256 if GPU allows
images = []
for img_path in sorted(glob.glob(os.path.join(DATA_DIR, "images", "*.png"))):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
    images.append(img)

images = np.array(images, dtype=np.float32) / 255.0
print("Subset ready. Shape:", images.shape)

# ================================
# Autoencoder-based Image Compression
# ================================

import os, glob, requests, zipfile, shutil, cv2
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

# --------------------
# 1. Download & prepare DIV2K subset
# --------------------
DATA_DIR = "./DIV2K_subset"
os.makedirs(DATA_DIR, exist_ok=True)

url = "http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip"
zip_path = "DIV2K_train_HR.zip"

if not os.path.exists(os.path.join(DATA_DIR, "images")):
    print("Downloading DIV2K (~1GB, may take time)...")
    r = requests.get(url, stream=True)
    with open(zip_path, "wb") as f:
        for chunk in r.iter_content(chunk_size=1024*1024):
            f.write(chunk)
    print("Extracting...")
    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall("./DIV2K_raw")
    os.remove(zip_path)

    # Take only first 100 images for subset
    src_folder = "./DIV2K_raw/DIV2K_train_HR"
    dst_folder = os.path.join(DATA_DIR, "images")
    os.makedirs(dst_folder, exist_ok=True)
    for i, f in enumerate(sorted(glob.glob(os.path.join(src_folder, "*.png")))[:100]):
        shutil.move(f, dst_folder)
    shutil.rmtree("./DIV2K_raw")

print("Dataset ready.")

# Preprocess
IMG_SIZE = 128  # reduce to 128x128 to avoid GPU OOM
images = []
for img_path in sorted(glob.glob(os.path.join(DATA_DIR, "images", "*.png"))):
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
    images.append(img)

images = np.array(images, dtype=np.float32) / 255.0
print("Final dataset shape:", images.shape)

# --------------------
# 2. Build Autoencoder
# --------------------
def build_autoencoder(latent_dim=256, input_shape=(IMG_SIZE, IMG_SIZE, 3)):
    # Encoder
    encoder_inputs = layers.Input(shape=input_shape)
    x = layers.Conv2D(64, 4, strides=2, padding='same', activation='relu')(encoder_inputs)
    x = layers.Conv2D(128, 4, strides=2, padding='same', activation='relu')(x)
    x = layers.Conv2D(256, 4, strides=2, padding='same', activation='relu')(x)
    x = layers.Flatten()(x)
    latent = layers.Dense(latent_dim, name="latent")(x)

    # Decoder
    x = layers.Dense(16*16*256, activation='relu')(latent)
    x = layers.Reshape((16,16,256))(x)
    x = layers.Conv2DTranspose(128, 4, strides=2, padding='same', activation='relu')(x)
    x = layers.Conv2DTranspose(64, 4, strides=2, padding='same', activation='relu')(x)
    x = layers.Conv2DTranspose(32, 4, strides=2, padding='same', activation='relu')(x)
    decoder_outputs = layers.Conv2D(3, 3, activation='sigmoid', padding='same')(x)

    return models.Model(encoder_inputs, decoder_outputs, name="Autoencoder")

autoencoder = build_autoencoder(latent_dim=512)
autoencoder.summary()

# --------------------
# 3. Custom Loss (MSE + SSIM)
# --------------------
def custom_loss(y_true, y_pred):
    mse = tf.reduce_mean(tf.square(y_true - y_pred))
    ssim = tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))
    return mse + (1.0 - ssim)

autoencoder.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=custom_loss)

# --------------------
# 4. Training
# --------------------
history = autoencoder.fit(
    images, images,
    epochs=20,
    batch_size=8,
    shuffle=True
)

# --------------------
# 5. Test Reconstruction
# --------------------
n = 5
sample_imgs = images[:n]
recon_imgs = autoencoder.predict(sample_imgs)

plt.figure(figsize=(12,6))
for i in range(n):
    # Original
    plt.subplot(2, n, i+1)
    plt.imshow(sample_imgs[i])
    plt.axis("off")
    if i==0: plt.title("Original")

    # Reconstructed
    plt.subplot(2, n, n+i+1)
    plt.imshow(recon_imgs[i])
    plt.axis("off")
    if i==0: plt.title("Reconstructed")

plt.show()

import os, glob, requests, zipfile, shutil
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import VGG19
from tensorflow.keras.models import Model
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image

# ------------------------
# Download DIV2K Dataset (HR)
# ------------------------
DIV2K_DIR = "./DIV2K_train_HR"
os.makedirs(DIV2K_DIR, exist_ok=True)
ZIP_PATH = "./DIV2K_train_HR.zip"

if len(glob.glob(os.path.join(DIV2K_DIR, "*.png"))) < 100:
    print("Downloading DIV2K HR (~1GB)...")
    url = "http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip"
    r = requests.get(url, stream=True)
    with open(ZIP_PATH, 'wb') as f:
        for chunk in r.iter_content(chunk_size=1024*1024):
            if chunk:
                f.write(chunk)
    print("Download finished. Extracting...")
    with zipfile.ZipFile(ZIP_PATH, 'r') as zip_ref:
        zip_ref.extractall(DIV2K_DIR)
    os.remove(ZIP_PATH)

    # Flatten folder structure
    nested_folder = os.path.join(DIV2K_DIR, "DIV2K_train_HR")
    if os.path.exists(nested_folder):
        for f in glob.glob(os.path.join(nested_folder,"*.*")):
            shutil.move(f, DIV2K_DIR)
        os.rmdir(nested_folder)

# ------------------------
# Load and Preprocess Images
# ------------------------
IMG_SIZE = 128
def load_image(path):
    img = Image.open(path).convert("RGB")
    img = img.resize((IMG_SIZE, IMG_SIZE))
    img = np.array(img).astype("float32") / 127.5 - 1.0  # Scale [-1,1]
    return img

image_paths = glob.glob(os.path.join(DIV2K_DIR, "*.png"))
print("Total DIV2K images:", len(image_paths))

dataset = np.array([load_image(p) for p in image_paths[:800]])  # Use subset
print("Dataset shape:", dataset.shape)

# Train / Test Split
split = int(0.9 * len(dataset))
x_train, x_test = dataset[:split], dataset[split:]

# ------------------------
# VGG19 Perceptual Loss
# ------------------------
vgg = VGG19(include_top=False, weights="imagenet", input_shape=(IMG_SIZE, IMG_SIZE, 3))
vgg.trainable = False
feature_extractor = Model(inputs=vgg.input, outputs=vgg.get_layer("block3_conv3").output)

def perceptual_loss(y_true, y_pred):
    true_features = feature_extractor(y_true)
    pred_features = feature_extractor(y_pred)
    return tf.reduce_mean(tf.square(true_features - pred_features))

# ------------------------
# Encoder
# ------------------------
LATENT_DIM = 8192
def build_encoder(latent_dim):
    inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
    x = layers.Conv2D(64, 3, strides=2, padding="same", activation="relu")(inputs)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(128, 3, strides=2, padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(256, 3, strides=2, padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Flatten()(x)
    latent = layers.Dense(latent_dim, activation="linear")(x)
    return models.Model(inputs, latent, name="encoder")

# ------------------------
# Decoder
# ------------------------
def build_decoder(latent_dim):
    latent_inputs = layers.Input(shape=(latent_dim,))
    x = layers.Dense(16*16*256, activation="relu")(latent_inputs)
    x = layers.Reshape((16, 16, 256))(x)
    x = layers.Conv2DTranspose(256, 3, strides=2, padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2DTranspose(128, 3, strides=2, padding="same", activation="relu")(x)
    x = layers.BatchNormalization()(x)
    x = layers.Conv2DTranspose(64, 3, strides=2, padding="same", activation="relu")(x)
    x = layers.Conv2DTranspose(3, 3, activation="tanh", padding="same")(x)  # Output [-1,1]
    return models.Model(latent_inputs, x, name="decoder")

# ------------------------
# Autoencoder
# ------------------------
encoder = build_encoder(LATENT_DIM)
decoder = build_decoder(LATENT_DIM)

inputs = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
latent = encoder(inputs)
outputs = decoder(latent)
autoencoder = models.Model(inputs, outputs, name="autoencoder")

# Custom loss
def custom_loss(y_true, y_pred):
    mse = tf.reduce_mean(tf.square(y_true - y_pred))
    ssim_loss = 1 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=2.0))
    perceptual = perceptual_loss(y_true, y_pred)
    return mse + 0.5 * ssim_loss + 0.1 * perceptual

autoencoder.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=custom_loss)

# ------------------------
# Training
# ------------------------
history = autoencoder.fit(
    x_train, x_train,
    epochs=50,
    batch_size=16,
    shuffle=True,
    validation_data=(x_test, x_test),
    callbacks=[tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True)]
)

# ------------------------
# Reconstruction Demo
# ------------------------
decoded_imgs = autoencoder.predict(x_test[:10])

plt.figure(figsize=(20,4))
for i in range(10):
    # Original
    ax = plt.subplot(2, 10, i+1)
    plt.imshow((x_test[i] + 1) / 2)
    plt.axis("off")

    # Reconstructed
    ax = plt.subplot(2, 10, i+1+10)
    plt.imshow((decoded_imgs[i] + 1) / 2)
    plt.axis("off")

plt.show()

# Memory-efficient DIV2K Autoencoder training (SSIM + VGG perceptual)
# Requirements: tensorflow, numpy, matplotlib, Pillow (PIL), scikit-image, opencv-python
# Assumes DIV2K images are already present in ./DIV2K_train_HR (800 pngs)

import os, glob
import numpy as np
from PIL import Image
import tensorflow as tf
from tensorflow.keras import layers, models
from tensorflow.keras.applications import VGG19, vgg19
from tensorflow.keras.models import Model
import matplotlib.pyplot as plt
from skimage.metrics import peak_signal_noise_ratio as sk_psnr
from skimage.metrics import structural_similarity as sk_ssim

# ----- Config -----
DIV2K_DIR = "./DIV2K_train_HR"   # should contain *.png images
IMG_SIZE = 128                    # 128 is a good trade-off; increase if you have memory
BATCH_SIZE = 8                    # lower if you hit OOM
LATENT_DIM = 1024                 # small but effective; increase if memory allows
EPOCHS = 50
USE_PERCEPTUAL = True             # set False to disable VGG perceptual loss if OOM

# Optional: enable mixed precision to reduce memory and speed up on modern GPUs
# (works well on NVIDIA GPUs with recent TF versions)
try:
    from tensorflow.keras.mixed_precision import experimental as mixed_precision
    policy = mixed_precision.Policy('mixed_float16')
    mixed_precision.set_policy(policy)
    print("Mixed precision enabled.")
except Exception:
    # new TF versions use tf.keras.mixed_precision.set_global_policy('mixed_float16')
    try:
        tf.keras.mixed_precision.set_global_policy('mixed_float16')
        print("Mixed precision enabled (new API).")
    except Exception:
        print("Mixed precision not enabled (optional).")

# ----- Data loader using tf.data for efficiency -----
def load_and_preprocess(path):
    # load with PIL, resize, convert range to [-1,1]
    img = Image.open(path).convert("RGB")
    img = img.resize((IMG_SIZE, IMG_SIZE), Image.LANCZOS)
    arr = np.array(img).astype(np.float32) / 127.5 - 1.0   # [-1,1]
    return arr

img_paths = sorted(glob.glob(os.path.join(DIV2K_DIR, "*.png")))
if len(img_paths) == 0:
    raise RuntimeError(f"No images found in {DIV2K_DIR}. Put DIV2K PNGs there or change path.")

# Use a subset for faster debugging; set to len(img_paths) for all 800
NUM_IMAGES = min(len(img_paths), 800)
img_paths = img_paths[:NUM_IMAGES]
print(f"Using {len(img_paths)} images. IMG_SIZE={IMG_SIZE}, BATCH={BATCH_SIZE}, LATENT={LATENT_DIM}")

def gen():
    for p in img_paths:
        yield load_and_preprocess(p)

dataset = tf.data.Dataset.from_generator(
    gen,
    output_types=tf.float32,
    output_shapes=(IMG_SIZE, IMG_SIZE, 3)
)
dataset = dataset.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)
# split into train/test via take/skip
total_batches = int(np.ceil(NUM_IMAGES / BATCH_SIZE))
val_batches = max(1, int(0.1 * total_batches))
train_dataset = dataset.skip(val_batches)
val_dataset = dataset.take(val_batches)

# ----- VGG perceptual feature extractor (optional) -----
if USE_PERCEPTUAL:
    # VGG expects inputs in 'RGB' with pixel values in range 0..255 and special preprocessing
    vgg = VGG19(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))
    vgg.trainable = False
    # use a relatively shallow layer to reduce memory (block2 or block3)
    # block3_conv3 is heavier; block2_conv2 is lighter
    feat_layer = 'block3_conv3' if IMG_SIZE >= 128 else 'block2_conv2'
    feature_extractor = Model(inputs=vgg.input, outputs=vgg.get_layer(feat_layer).output)
else:
    feature_extractor = None

# ----- Encoder/Decoder (GAP-based to avoid massive Dense) -----
def build_encoder(latent_dim=LATENT_DIM):
    inp = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
    x = layers.Conv2D(64, 3, strides=2, padding='same', activation='relu')(inp)   # /2
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(128, 3, strides=2, padding='same', activation='relu')(x)    # /4
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(256, 3, strides=2, padding='same', activation='relu')(x)    # /8
    x = layers.BatchNormalization()(x)
    x = layers.Conv2D(512, 3, strides=2, padding='same', activation='relu')(x)    # /16
    x = layers.BatchNormalization()(x)
    x = layers.GlobalAveragePooling2D()(x)   # -> 512
    latent = layers.Dense(latent_dim, name='latent')(x)   # 512 -> latent_dim
    return models.Model(inp, latent, name='encoder')

def build_decoder(latent_dim=LATENT_DIM):
    latent_in = layers.Input(shape=(latent_dim,))
    x = layers.Dense(8*8*256, activation='relu')(latent_in)   # small spatial start
    x = layers.Reshape((8,8,256))(x)
    x = layers.Conv2DTranspose(256, 3, strides=2, padding='same', activation='relu')(x)  # 16
    x = layers.BatchNormalization()(x)
    x = layers.Conv2DTranspose(128, 3, strides=2, padding='same', activation='relu')(x)  # 32
    x = layers.BatchNormalization()(x)
    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same', activation='relu')(x)   # 64
    x = layers.BatchNormalization()(x)
    x = layers.Conv2DTranspose(32, 3, strides=2, padding='same', activation='relu')(x)   # 128
    x = layers.Conv2D(3, 3, padding='same', activation='tanh')(x)  # output in [-1,1]
    return models.Model(latent_in, x, name='decoder')

encoder = build_encoder(LATENT_DIM)
decoder = build_decoder(LATENT_DIM)

inp = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))
z = encoder(inp)
out = decoder(z)
autoencoder = models.Model(inp, out, name='autoencoder')

# ----- Loss functions -----
# Note: autoencoder outputs in [-1,1], so ssim max_val=2.0
mse_loss_fn = tf.keras.losses.MeanSquaredError()

def compute_perceptual(y_true, y_pred):
    # Convert from [-1,1] to VGG-friendly input: [0,255] with vgg preprocess
    y_true_vgg = (y_true + 1.0) * 127.5
    y_pred_vgg = (y_pred + 1.0) * 127.5
    # preprocess for vgg
    y_true_vgg = vgg19.preprocess_input(y_true_vgg)
    y_pred_vgg = vgg19.preprocess_input(y_pred_vgg)
    f_true = feature_extractor(y_true_vgg)
    f_pred = feature_extractor(y_pred_vgg)
    return tf.reduce_mean(tf.square(f_true - f_pred))

@tf.function
def loss_fn(y_true, y_pred):
    mse = mse_loss_fn(y_true, y_pred)
    ssim_l = 1.0 - tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=2.0))
    if USE_PERCEPTUAL:
        perc = compute_perceptual(y_true, y_pred)
    else:
        perc = 0.0
    # weights: tune these
    return mse + 0.5 * ssim_l + 0.1 * perc

# Compile with custom training loop or wrapper using a dummy loss and custom step
autoencoder.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=loss_fn)

# ----- Training -----
callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True)
]
history = autoencoder.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=EPOCHS,
    callbacks=callbacks,
    verbose=1
)

# ----- Evaluate & visualize a few examples -----
def evaluate_and_show(n=6):
    # sample n from val_dataset
    batch = next(iter(val_dataset))
    originals = batch[:n].numpy()
    recon = autoencoder.predict(batch[:n])
    # convert to [0,1]
    originals_disp = (originals + 1.0) / 2.0
    recon_disp = (recon + 1.0) / 2.0

    plt.figure(figsize=(12, 4))
    for i in range(n):
        plt.subplot(2, n, i+1)
        plt.imshow(np.clip(originals_disp[i], 0, 1))
        plt.axis('off')
        if i==0: plt.title('Original')
        plt.subplot(2, n, n+i+1)
        plt.imshow(np.clip(recon_disp[i], 0, 1))
        plt.axis('off')
        if i==0: plt.title('Reconstructed')
    plt.show()

evaluate_and_show(6)

import os, glob
import tensorflow as tf
from tensorflow.keras import layers, models

# ------------------------
# CONFIG
# ------------------------
IMG_SIZE = 128
LATENT_DIM = 512     # keep smaller for Colab
BATCH_SIZE = 4       # lower batch to save memory
EPOCHS = 20

# ------------------------
# DATASET LOADING
# ------------------------
DIV2K_DIR = "./DIV2K_train_HR"
train_images = glob.glob(os.path.join(DIV2K_DIR, "*.png"))
print("Total images found:", len(train_images))

def preprocess_image(path):
    img = tf.io.read_file(path)
    img = tf.image.decode_png(img, channels=3)
    img = tf.image.resize(img, (IMG_SIZE, IMG_SIZE))
    img = tf.cast(img, tf.float32) / 255.0
    return img, img   # input and target same

dataset = tf.data.Dataset.from_tensor_slices(train_images)
dataset = dataset.map(preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)
dataset = dataset.shuffle(200).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

# Train/val split
train_size = int(0.9 * len(train_images))
train_dataset = dataset.take(train_size // BATCH_SIZE)
val_dataset = dataset.skip(train_size // BATCH_SIZE)

# ------------------------
# AUTOENCODER MODEL
# ------------------------
def build_autoencoder(latent_dim=LATENT_DIM, input_shape=(IMG_SIZE, IMG_SIZE, 3)):
    inputs = layers.Input(shape=input_shape)

    # Encoder
    x = layers.Conv2D(64, 4, strides=2, padding='same', activation='relu')(inputs)
    x = layers.Conv2D(128, 4, strides=2, padding='same', activation='relu')(x)
    x = layers.Conv2D(256, 4, strides=2, padding='same', activation='relu')(x)
    x = layers.Flatten()(x)
    latent = layers.Dense(latent_dim, name="latent")(x)

    # Decoder
    x = layers.Dense((IMG_SIZE // 8) * (IMG_SIZE // 8) * 256, activation='relu')(latent)
    x = layers.Reshape((IMG_SIZE // 8, IMG_SIZE // 8, 256))(x)
    x = layers.Conv2DTranspose(128, 4, strides=2, padding='same', activation='relu')(x)
    x = layers.Conv2DTranspose(64, 4, strides=2, padding='same', activation='relu')(x)
    outputs = layers.Conv2DTranspose(3, 4, strides=2, padding='same', activation='sigmoid')(x)

    return models.Model(inputs, outputs)

autoencoder = build_autoencoder(LATENT_DIM)
autoencoder.summary()

# ------------------------
# LOSS + OPTIMIZER
# ------------------------
def custom_loss(y_true, y_pred):
    mse = tf.reduce_mean(tf.square(y_true - y_pred))
    ssim = tf.reduce_mean(tf.image.ssim(y_true, y_pred, max_val=1.0))
    return mse + (1.0 - ssim)

autoencoder.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=custom_loss)

# ------------------------
# TRAINING
# ------------------------
callbacks = [
    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=4, restore_best_weights=True)
]

history = autoencoder.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=EPOCHS,
    callbacks=callbacks
)

import matplotlib.pyplot as plt
import cv2
import numpy as np

# Pick a few test images (you can also load from validation set instead of train_images)
test_files = train_images[:5]   # make sure these are paths, not already loaded arrays
test_imgs = [cv2.imread(f) for f in test_files]
test_imgs = [cv2.cvtColor(img, cv2.COLOR_BGR2RGB) for img in test_imgs]
test_imgs = [cv2.resize(img, (IMG_SIZE, IMG_SIZE)) for img in test_imgs]

x_test = np.array(test_imgs, dtype=np.float32) / 255.0

# Run autoencoder
reconstructed = autoencoder.predict(x_test)

# Clip values to [0,1] and convert to float32
x_test_disp = np.clip(x_test, 0, 1).astype(np.float32)
reconstructed_disp = np.clip(reconstructed, 0, 1).astype(np.float32)

# Helper to visualize
def show_images(originals, reconstructions, n=5):
    plt.figure(figsize=(15,6))

    for i in range(n):
        # Original
        ax = plt.subplot(2, n, i+1)
        plt.imshow(originals[i])
        plt.title("Original")
        plt.axis("off")

        # Reconstructed
        ax = plt.subplot(2, n, i+1+n)
        plt.imshow(reconstructions[i])
        plt.title("Reconstructed")
        plt.axis("off")

    plt.show()

show_images(x_test_disp, reconstructed_disp)

import numpy as np
import matplotlib.pyplot as plt
import cv2
from tensorflow.keras import Model
import tensorflow as tf

# --------------------------
# 1. Build encoder & decoder
# --------------------------
# Extract encoder (up to latent layer)
encoder_input = autoencoder.input
encoder_output = autoencoder.get_layer("latent").output   # your latent layer must be named "latent"
encoder = Model(encoder_input, encoder_output)

# Build decoder (from latent back to image)
latent_input = tf.keras.Input(shape=(encoder_output.shape[1],))
x = latent_input
start = False
for layer in autoencoder.layers:
    if start:
        x = layer(x)
    if layer.name == "latent":
        start = True
decoder = Model(latent_input, x)

# --------------------------
# 2. Compression functions
# --------------------------
def compress_autoencoder(img, encoder, decoder):
    h, w, c = img.shape
    orig_size = img.nbytes

    # Normalize
    x = np.expand_dims(img.astype(np.float32) / 255.0, axis=0)

    # Encode → latent
    z = encoder.predict(x)
    latent_size = z.nbytes

    # Decode → reconstruct
    x_hat = decoder.predict(z)[0]
    x_hat = np.clip(x_hat * 255.0, 0, 255).astype(np.uint8)

    compression_ratio = orig_size / latent_size
    return x_hat, compression_ratio

def compress_jpeg(img, quality=50):
    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]
    result, encimg = cv2.imencode('.jpg', img, encode_param)
    compressed_size = len(encimg)
    decimg = cv2.imdecode(encimg, 1)
    compression_ratio = img.nbytes / compressed_size
    return decimg, compression_ratio

def compress_jpeg2000(img, quality=500):
    encode_param = [int(cv2.IMWRITE_JPEG2000_COMPRESSION_X1000), quality]
    result, encimg = cv2.imencode('.jp2', img, encode_param)
    compressed_size = len(encimg)
    decimg = cv2.imdecode(encimg, 1)
    compression_ratio = img.nbytes / compressed_size
    return decimg, compression_ratio

# --------------------------
# 3. Comparison Function
# --------------------------
def compare_methods(img, encoder, decoder):
    orig_size_kb = img.nbytes / 1024

    ae_img, ae_ratio = compress_autoencoder(img, encoder, decoder)
    jpeg_img, jpeg_ratio = compress_jpeg(img, quality=50)
    jp2_img, jp2_ratio = compress_jpeg2000(img, quality=500)

    titles = [
        f"Original ({orig_size_kb:.1f} KB)",
        f"Autoencoder (x{ae_ratio:.2f})",
        f"JPEG (x{jpeg_ratio:.2f})",
        f"JPEG2000 (x{jp2_ratio:.2f})"
    ]
    images = [img, ae_img, jpeg_img, jp2_img]

    plt.figure(figsize=(16,6))
    for i in range(4):
        ax = plt.subplot(1,4,i+1)
        plt.imshow(cv2.cvtColor(images[i], cv2.COLOR_BGR2RGB))
        plt.title(titles[i])
        plt.axis("off")
    plt.show()

# --------------------------
# 4. Test on a DIV2K image
# --------------------------
test_img = (x_test_disp[0] * 255).astype(np.uint8)   # first image from test set
compare_methods(test_img, encoder, decoder)

